{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFtMlg5QVu4N"
   },
   "source": [
    "<center>\n",
    "\n",
    "## Creation of Hydbrid Linformer Transformer U-Net Architecture for Accurate Brain Tumor Segmentation  (using Autocasting)     \n",
    "\n",
    "\n",
    "#### By Lawrence Menegus\n",
    "\n",
    "\n",
    "Note: This Notebook was used in the Publication \"LINTUNET: A HYBRID TRANSFORMER-CNN ARCHITECTURE FOR BRAIN TUMOR SEGMENTATION\"\n",
    "\n",
    "This project was conducted on Google Colab, utilizing Google Cloud's computational resources. The results presented below are based on one of several trial runs of the Jupyter notebook. Due to variability in cloud resource allocation, minor differences may occur between runs. However, LinTUNet consistently outperformed U-Net in all trials, but with the degree of improvement varying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM629I68ysKf"
   },
   "source": [
    "To run this notebook you need to upload the zipped Data_Internship folder to the google drive. Once uploaded you can run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9kmkziC6kP2"
   },
   "source": [
    "You must install packages by the following Command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71058,
     "status": "ok",
     "timestamp": 1739541528772,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "w8bmhWsp7ZEU",
    "outputId": "3cb7975b-a1b3-4fa0-a203-1ed7ab05103a"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib opencv-python-headless pillow torch torchvision scikit-image tifffile tqdm torchsummary linformer ace-tools-open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9570,
     "status": "ok",
     "timestamp": 1739541538334,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "XeXDwnSXE5DR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from linformer import Linformer\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "import skimage.draw\n",
    "import shutil\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ace_tools_open as tools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXahMp3VH973"
   },
   "source": [
    "Mount Google Drive for CoLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23935,
     "status": "ok",
     "timestamp": 1739541562262,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "XtUcts3KE9Ra",
    "outputId": "34aefff9-7a0e-4512-9ed4-0e396b121b59"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cu7hwPFbxJpO"
   },
   "source": [
    "Lists all the files and folders in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1739541562435,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "58hJ52gMxG1R",
    "outputId": "d2ba20b4-aa38-45f3-874f-4a4aebe60f2b"
   },
   "outputs": [],
   "source": [
    "ls /content/drive/MyDrive/Publication/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pmHOnMzxRxA"
   },
   "source": [
    "Unzip file for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1739541562436,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "LWipVd0qRUkh",
    "outputId": "d2bef136-c03d-4e00-f8e4-ffd6a52cec23"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "if os.path.exists('/content/drive/MyDrive/Publication/test'):\n",
    "  print(\"skipping\")\n",
    "if os.path.exists('/content/drive/MyDrive/Publication/train'):\n",
    "  print(\"skipping\")\n",
    "if os.path.exists('/content/drive/MyDrive/Publication/valid'):\n",
    "  print(\"skipping\")\n",
    "\n",
    "else:\n",
    "  zip_path = \"/content/drive/MyDrive/Publication/Data_Internship.zip\"\n",
    "  extract_path = '/content/drive/MyDrive/Publication/'\n",
    "\n",
    "  os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(extract_path)\n",
    "\n",
    "  print(\"Dataset unzipped successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1739541562582,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "_jG5s82nx01z",
    "outputId": "39b69f55-692e-41aa-f226-f3fd96001fa1"
   },
   "outputs": [],
   "source": [
    "ls /content/drive/MyDrive/Publication/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G93zIQm1x3Ex"
   },
   "source": [
    "you should be able to see the the valid train and test folders in\n",
    "the google drive. these will be used for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZPn68bojlmg"
   },
   "source": [
    "### Create Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1739542519295,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "9ESaS545jbTb"
   },
   "outputs": [],
   "source": [
    "# Define base directory paths for training, testing, and validation datasets\n",
    "BASE_DIR = '/content/drive/MyDrive/Publication/'\n",
    "train_path = os.path.join(BASE_DIR, \"train/\")\n",
    "test_path = os.path.join(BASE_DIR, \"test/\")\n",
    "valid_path = os.path.join(BASE_DIR, \"valid/\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the COCO annotation file containing image segmentation data\n",
    "with open('/content/drive/MyDrive/Publication/valid/_annotations.coco.json', 'r') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq_obzhxE5DP"
   },
   "source": [
    "<center>\n",
    "\n",
    "## The Application of U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfWUXvwrE5DQ"
   },
   "source": [
    "\n",
    "### Brain Tumor Dataset\n",
    "\n",
    "From [Kaggle](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb4o_4H2Vu4U"
   },
   "source": [
    "#### Print Data **Annoations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1739542521418,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "qf33AY2BVu4U",
    "outputId": "dea06168-b78d-420d-cd2a-528fe96be798"
   },
   "outputs": [],
   "source": [
    "# Print categories from the COCO annotation file\n",
    "data['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOevSmWAVu4X"
   },
   "source": [
    "### Creating Masks for the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1739542523828,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "1QzUysYEVu4Y"
   },
   "outputs": [],
   "source": [
    "def create_mask(image_info, annotations, output_folder, max_print=3):\n",
    "\n",
    "    # Initialize an empty mask with the same dimensions as the image\n",
    "    mask_np = np.zeros((image_info['height'], image_info['width']), dtype=np.uint8)\n",
    "    printed_masks = 0\n",
    "\n",
    "    for ann in annotations:\n",
    "\n",
    "        # Only create masks for the current image\n",
    "        if ann['image_id'] == image_info['id']:\n",
    "            for seg_idx, seg in enumerate(ann['segmentation']):\n",
    "\n",
    "                # Generate the polygon points for the segmentation\n",
    "                rr, cc = skimage.draw.polygon(seg[1::2], seg[0::2], mask_np.shape)\n",
    "                seg_mask = np.zeros_like(mask_np, dtype=np.uint8)\n",
    "\n",
    "                # Fill the polygon with white (255) on the mask\n",
    "                seg_mask[rr, cc] = 255\n",
    "\n",
    "                # Save the mask as a TIFF file\n",
    "                mask_path = os.path.join(output_folder, f\"{image_info['file_name'].replace('.jpg', '')}_seg_{seg_idx}.tif\")\n",
    "                tifffile.imwrite(mask_path, seg_mask)\n",
    "                printed_masks += 1\n",
    "\n",
    "                #Stops the function\n",
    "                if printed_masks >= max_print:\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9k0BsHlVu4Y"
   },
   "source": [
    "#### Creating a folder directory of Masks and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1739542526213,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "G9gTk2UwVu4Y"
   },
   "outputs": [],
   "source": [
    "def mask_folders(json_file, mask_output_folder, image_output_folder, original_image_dir):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    images = data['images']\n",
    "    annotations = data['annotations']\n",
    "\n",
    "    # Create directories for masks and images if they don't exist\n",
    "    os.makedirs(mask_output_folder, exist_ok=True)\n",
    "    os.makedirs(image_output_folder, exist_ok=True)\n",
    "\n",
    "    # Create masks and copy the original images to the new directories\n",
    "    for img in images:\n",
    "        create_mask(img, annotations, mask_output_folder)\n",
    "        original_image_path = os.path.join(original_image_dir, img['file_name'])\n",
    "\n",
    "        # Check if the image file exists before trying to copy it\n",
    "        if os.path.exists(original_image_path):\n",
    "            new_image_path = os.path.join(image_output_folder, os.path.basename(original_image_path))\n",
    "            shutil.copy2(original_image_path, new_image_path)\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {original_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n79kSWTEVu4Z"
   },
   "source": [
    "#### Organize the train, test, and validation datasets into their respective folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1739542528710,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "kRRgOalL8Lf8",
    "outputId": "810de208-c892-456a-ef93-3690b87a8e7a"
   },
   "outputs": [],
   "source": [
    "def mask_folders_if_not_exist(json_file, mask_output_folder, image_output_folder, original_image_dir):\n",
    "\n",
    "    # Check if the output folders exist\n",
    "    if not os.path.exists(mask_output_folder) or not os.path.exists(image_output_folder):\n",
    "        # Create output folders if they don't exist\n",
    "        os.makedirs(mask_output_folder, exist_ok=True)\n",
    "        os.makedirs(image_output_folder, exist_ok=True)\n",
    "\n",
    "        # Process the images and masks\n",
    "        # Update this section with a file existence check inside mask_folders()\n",
    "        mask_folders(json_file, mask_output_folder, image_output_folder, original_image_dir)\n",
    "    else:\n",
    "        print(f\"Folders {mask_output_folder} and {image_output_folder} already exist, skipping processing.\")\n",
    "\n",
    "# For 'test' dataset\n",
    "original_image_dir = '/content/drive/MyDrive/Publication/test'\n",
    "json_file = '/content/drive/MyDrive/Publication/test/_annotations.coco.json'\n",
    "mask_output_folder = '/content/drive/MyDrive/Publication/test_final/masks'\n",
    "image_output_folder = '/content/drive/MyDrive/Publication/test_final/images'\n",
    "mask_folders_if_not_exist(json_file, mask_output_folder, image_output_folder, original_image_dir)\n",
    "\n",
    "# For 'train' dataset\n",
    "original_image_dir = '/content/drive/MyDrive/Publication/train'\n",
    "json_file = '/content/drive/MyDrive/Publication/train/_annotations.coco.json'\n",
    "mask_output_folder = '/content/drive/MyDrive/Publication/train_final/masks'\n",
    "image_output_folder = '/content/drive/MyDrive/Publication/train_final/images'\n",
    "mask_folders_if_not_exist(json_file, mask_output_folder, image_output_folder, original_image_dir)\n",
    "\n",
    "# For 'valid' dataset\n",
    "original_image_dir = '/content/drive/MyDrive/Publication/valid'\n",
    "json_file = '/content/drive/MyDrive/Publication/valid/_annotations.coco.json'\n",
    "mask_output_folder = '/content/drive/MyDrive/Publication/valid_final/masks'\n",
    "image_output_folder = '/content/drive/MyDrive/Publication/valid_final/images'\n",
    "mask_folders_if_not_exist(json_file, mask_output_folder, image_output_folder, original_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79hqQ2ZhB7_U"
   },
   "source": [
    "#### Compares two folders and delete Unmatched items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18946,
     "status": "ok",
     "timestamp": 1739542549459,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "1SZe74PrB5Os"
   },
   "outputs": [],
   "source": [
    "def compare_folders(folder1_path, folder2_path):\n",
    "\n",
    "    folder1_items = os.listdir(folder1_path)\n",
    "    folder2_items = os.listdir(folder2_path)\n",
    "\n",
    "    # Compare items in folder1 with those in folder2 and delete unmatched items\n",
    "    for item1 in folder1_items:\n",
    "        found = False\n",
    "        for item2 in folder2_items:\n",
    "            if item1[:4] == item2[:4]:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"Corresponding item for {item1} not found.\")\n",
    "            item1_path = os.path.join(folder1_path, item1)\n",
    "            os.remove(item1_path)\n",
    "            print(f\"Deleted {item1}\")\n",
    "\n",
    "    # Compare items in folder2 with those in folder1 and delete unmatched items\n",
    "    for item2 in folder2_items:\n",
    "        found = False\n",
    "        for item1 in folder1_items:\n",
    "            if item2[:4] == item1[:4]:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"Corresponding item for {item2} not found.\")\n",
    "            item2_path = os.path.join(folder2_path, item2)\n",
    "            os.remove(item2_path)\n",
    "            print(f\"Deleted {item2}\")\n",
    "\n",
    "# Compare and clean up mismatched images and masks in the train folder\n",
    "folder1_path = \"/content/drive/MyDrive/Publication/train_final/images\"\n",
    "folder2_path = \"/content/drive/MyDrive/Publication/train_final/masks\"\n",
    "compare_folders(folder1_path, folder2_path)\n",
    "\n",
    "folder3_path = \"/content/drive/MyDrive/Publication/test_final/images\"\n",
    "folder4_path = \"/content/drive/MyDrive/Publication/test_final/masks\"\n",
    "compare_folders(folder3_path, folder4_path)\n",
    "\n",
    "folder5_path = \"/content/drive/MyDrive/Publication/valid_final/images\"\n",
    "folder6_path = \"/content/drive/MyDrive/Publication/valid_final/masks\"\n",
    "compare_folders(folder5_path, folder6_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739542549460,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "Us94WgVFW5_p"
   },
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(image_folder, mask_folder, test_size=0.2, valid_size=0.1):\n",
    "    # List all image and mask files\n",
    "    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "    mask_files = sorted([f for f in os.listdir(mask_folder) if f.endswith('.tif') or f.endswith('.png')])\n",
    "\n",
    "    # Ensure the images and masks are matched correctly by their file names\n",
    "    assert len(image_files) == len(mask_files), \"Mismatch between image and mask files\"\n",
    "\n",
    "    # Split the data into train and remaining (test + validation)\n",
    "    train_images, remaining_images, train_masks, remaining_masks = train_test_split(\n",
    "        image_files, mask_files, test_size=test_size + valid_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Split the remaining data into validation and test\n",
    "    valid_images, test_images, valid_masks, test_masks = train_test_split(\n",
    "        remaining_images, remaining_masks, test_size=test_size / (test_size + valid_size), random_state=42\n",
    "    )\n",
    "\n",
    "    return train_images, valid_images, test_images, train_masks, valid_masks, test_masks\n",
    "\n",
    "\n",
    "# Apply the split function to your data\n",
    "train_images, valid_images, test_images, train_masks, valid_masks, test_masks = split_data(\n",
    "    image_folder=\"/content/drive/MyDrive/Publication/train_final/images\",\n",
    "    mask_folder=\"/content/drive/MyDrive/Publication/train_final/masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrfKk5PMOcLD"
   },
   "source": [
    "Create Custom function to load\n",
    " masks and images to the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739542549461,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "FirJ_U3UI5pQ"
   },
   "outputs": [],
   "source": [
    "class loading_images(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_folder = os.path.join(root_dir, \"images\")\n",
    "        self.mask_folder = os.path.join(root_dir, \"masks\")\n",
    "        self.image_files = sorted(os.listdir(self.image_folder))\n",
    "        self.mask_files = sorted(os.listdir(self.mask_folder))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "\n",
    "        #Load image and converts to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_gray = image.convert(\"L\")\n",
    "        mask_name = self.mask_files[idx]\n",
    "        mask_path = os.path.join(self.mask_folder, mask_name)\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        #Applying Transformations the image and mask\n",
    "        if self.transform:\n",
    "            image_gray = self.transform(image_gray)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image_gray, mask\n",
    "\n",
    "# Image Transformations for images\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Lambda(lambda x: x.clamp(0, 1))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdqHHq82I5R8"
   },
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1739542549461,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "Ve3x8iSbI5hO",
    "outputId": "46719764-9961-4638-956f-f61a84259156"
   },
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_path = \"/content/drive/MyDrive/Publication/train_final\"\n",
    "valid_path = \"/content/drive/MyDrive/Publication/valid_final\"\n",
    "test_path = \"/content/drive/MyDrive/Publication/test_final\"\n",
    "\n",
    "train_dataset = loading_images(train_path, transform=image_transform)\n",
    "valid_dataset = loading_images(valid_path, transform=image_transform)\n",
    "test_dataset = loading_images(test_path, transform=image_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, prefetch_factor=5)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4, prefetch_factor=5)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, prefetch_factor=5)\n",
    "\n",
    "# Print out some sample data to verify everything is working\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-zyHqnz1iPh"
   },
   "source": [
    "### COCO Annoations with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "executionInfo": {
     "elapsed": 3591,
     "status": "ok",
     "timestamp": 1739543540802,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "7xDLEfk1Vu4X",
    "outputId": "9bad2fe1-0562-46bd-d474-a000d3c0a99d"
   },
   "outputs": [],
   "source": [
    "#### Random Sample of MRI Brain Scan\n",
    "# Function to display a random sample of MRI images with overlaid segmentation masks\n",
    "def display_masks(image_names, data):\n",
    "\n",
    "    # Create a 2x2 grid for displaying images\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    for i, img_path in enumerate(image_names):\n",
    "        draw_image = cv2.imread(img_path)\n",
    "        img_filename = os.path.basename(img_path)\n",
    "\n",
    "        # Get the image ID from the annotation data based on the filename\n",
    "        img_id = [item for item in data['images'] if item['file_name'] == img_filename][0]['id']\n",
    "\n",
    "        # Get the corresponding annotations (segmentation points) for the image\n",
    "        img_annotations = [ann for ann in data['annotations'] if ann['image_id'] == img_id]\n",
    "        points_list = img_annotations[0]['segmentation']\n",
    "\n",
    "        # Convert points to a NumPy array and resphaping for drawling thepolygon\n",
    "        points = np.array(points_list, np.int32)\n",
    "        points = points.reshape((-1, 1, 2))\n",
    "\n",
    "        # Draw the segmentation polygon on the image\n",
    "        cv2.polylines(draw_image, [points], True, (0, 0, 255), 3)\n",
    "\n",
    "        # Display the image with the mask\n",
    "        ax[i // 2, i % 2].imshow(cv2.cvtColor(draw_image, cv2.COLOR_BGR2RGB))\n",
    "        ax[i // 2, i % 2].axis('off')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load and display a random sample of 4 MRI images with their masks\n",
    "with open('/content/drive/MyDrive/Publication/test/_annotations.coco.json', 'r') as file:\n",
    "    annotations = json.load(file)\n",
    "imge_dir = \"/content/drive/MyDrive/Publication/test\"\n",
    "all_image_files = [os.path.join(imge_dir, img['file_name']) for img in annotations['images']]\n",
    "random_image_files = random.sample(all_image_files, 4)\n",
    "display_masks(random_image_files, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxkxq3kc9ztQ"
   },
   "source": [
    "Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnjy9EdmXZlP"
   },
   "outputs": [],
   "source": [
    "# Define the Double Conv Block\n",
    "class DoubleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(F.relu(self.conv2(F.relu(self.conv1(x)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ86_3Xi969c"
   },
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nQ4C8W4XaQ6"
   },
   "outputs": [],
   "source": [
    "# Define the Downsample Block\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownsampleBlock, self).__init__()\n",
    "        self.double_conv = DoubleConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.double_conv(x)\n",
    "        p = self.pool(x)\n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcXzDHcI99Bj"
   },
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbL9SL4DXgmt"
   },
   "outputs": [],
   "source": [
    "# Define the Upsample Block\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.double_conv = DoubleConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat((x, skip_connection), dim=1)\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1OZlsvY9-pv"
   },
   "source": [
    "Unet Architecture (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnhclM1zXgXF"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = DownsampleBlock(in_channels, 64)\n",
    "        self.enc2 = DownsampleBlock(64, 128)\n",
    "        self.enc3 = DownsampleBlock(128, 256)\n",
    "        self.enc4 = DownsampleBlock(256, 512)\n",
    "        self.bottleneck = DoubleConvBlock(512, 1024)\n",
    "\n",
    "        self.dec4 = UpsampleBlock(1024, 512)\n",
    "        self.dec3 = UpsampleBlock(512, 256)\n",
    "        self.dec2 = UpsampleBlock(256, 128)\n",
    "        self.dec1 = UpsampleBlock(128, 64)\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1, p1 = self.enc1(x)\n",
    "        enc2, p2 = self.enc2(p1)\n",
    "        enc3, p3 = self.enc3(p2)\n",
    "        enc4, p4 = self.enc4(p3)\n",
    "        bottleneck = self.bottleneck(p4)\n",
    "        dec4 = self.dec4(bottleneck, enc4)\n",
    "        dec3 = self.dec3(dec4, enc3)\n",
    "        dec2 = self.dec2(dec3, enc2)\n",
    "        dec1 = self.dec1(dec2, enc1)\n",
    "        return self.final_conv(dec1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJuB0LiT9uv7"
   },
   "source": [
    "Summary of the Orginial CNN Unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1739407052735,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "NV1LAT7fmFCc",
    "outputId": "075b4204-861a-47fa-894d-3489da3d719f"
   },
   "outputs": [],
   "source": [
    "model = UNet(in_channels=1, out_channels=1).to(DEVICE)\n",
    "summary(model, (1, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egaj1bZ_km4i"
   },
   "source": [
    "Clears GPU Cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RbjDyihty0-"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGyRldjRk2C8"
   },
   "source": [
    "Defining  Optimizer, Scheduler and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8-4jJcC6Tvi"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG_f8KqoAgu8"
   },
   "source": [
    "Calculating the Total Parameters and the Total GPU Memory Allocation for the Models to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJURjJftdSR8"
   },
   "outputs": [],
   "source": [
    "def model_memory_usage(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        allocated_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "        print(f\"GPU Memory Allocated: {allocated_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HVF0liNBRzx"
   },
   "source": [
    "Calcuations for How many Seconds it takes to Process a Singular Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsoa2mJReIjM"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_time(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            start_time = time.time()\n",
    "            _ = model(images)\n",
    "            end_time = time.time()\n",
    "\n",
    "            total_time += (end_time - start_time)\n",
    "            num_samples += images.size(0)\n",
    "\n",
    "    avg_inference_time = total_time / num_samples\n",
    "    print(f\"Average inference time per image: {avg_inference_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZlnUH7SlGLZ"
   },
   "source": [
    "Calculates:\n",
    "- The Intersection over Union - How the Generated (predicted) Image Segmentation matches with the Grouund truth Image Segmenation\n",
    "\n",
    "- Accuracy\n",
    "- Percision\n",
    "- Recall\n",
    "- F1 Score   \n",
    "\n",
    "of all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAPwd8GidG84"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_iou(pred_mask, true_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_mask (torch.Tensor): The predicted binary segmentation mask (shape: [H, W])\n",
    "        true_mask (torch.Tensor): The ground truth binary segmentation mask (shape: [H, W])\n",
    "\n",
    "    Returns:\n",
    "        float: IoU score\n",
    "    \"\"\"\n",
    "    pred_mask = pred_mask > 0.5  # Threshold predictions\n",
    "    true_mask = true_mask > 0.5  # Ensure ground truth is binary\n",
    "\n",
    "    intersection = torch.logical_and(pred_mask, true_mask).sum().float()\n",
    "    union = torch.logical_or(pred_mask, true_mask).sum().float()\n",
    "\n",
    "    # If both masks are empty, IoU is 1 (perfect match)\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return (intersection / union).item()\n",
    "\n",
    "\n",
    "def calculate_metrics(outputs, masks):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, IoU, precision, recall, and F1-score for segmentation tasks.\n",
    "    \"\"\"\n",
    "    # Convert model outputs to binary predictions\n",
    "    y_pred_binary = (torch.sigmoid(outputs) > 0.5).float()  # Apply sigmoid & threshold\n",
    "\n",
    "    # **Ensure the ground truth is binary**\n",
    "    y_true_binary = (masks > 0.5).float()  # Convert ground truth to binary\n",
    "\n",
    "    # Compute Accuracy\n",
    "    correct = (y_pred_binary == masks).float().sum()\n",
    "    accuracy = correct / masks.numel()\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = calculate_iou(y_pred_binary, masks)\n",
    "\n",
    "    # Flatten the tensors for sklearn metrics\n",
    "    y_pred_np = y_pred_binary.cpu().detach().numpy().flatten()\n",
    "    y_true_np = y_true_binary.cpu().detach().numpy().flatten()\n",
    "\n",
    "    # Compute Precision, Recall, and F1-Score\n",
    "    precision = precision_score(y_true_np, y_pred_np, zero_division=1)\n",
    "    recall = recall_score(y_true_np, y_pred_np, zero_division=1)\n",
    "    f1 = f1_score(y_true_np, y_pred_np, zero_division=1)\n",
    "\n",
    "    return accuracy.item(), iou, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7aaWgMDEvnj"
   },
   "source": [
    "# Training the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQNLTvxdFC0A"
   },
   "source": [
    "Models' Training Process\n",
    "\n",
    "Uses:\n",
    "- Autocasting (Mixed percision) used for speeding up models  no effect on results.\n",
    "- Grad Scaling\n",
    "\n",
    "Calcuates per Epoch\n",
    "- Train and Valid Accuracy\n",
    "- Train and Valid Loss  \n",
    "- Train and Valid IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3ytxrQMFC0B"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    model.train()\n",
    "\n",
    "    train_losses, train_accuracies, train_ious, train_precisions, train_f1s = [], [], [], [], []\n",
    "    valid_losses, valid_accuracies, valid_ious, valid_precisions, valid_f1s = [], [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_train_accuracy, total_train_iou, total_train_precision, total_train_f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        num_batches_train = len(train_loader)\n",
    "\n",
    "        # Training phase\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE, dtype=torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "              # Forward pass: Move this line above metric calculation\n",
    "              outputs = model(images)\n",
    "\n",
    "              # Compute loss\n",
    "              loss = criterion(outputs, masks)\n",
    "\n",
    "              # Compute metrics\n",
    "              acc, iou, precision, recall, f1 = calculate_metrics(outputs, masks)\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update metrics\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_accuracy += acc\n",
    "            total_train_iou += iou\n",
    "            total_train_precision += precision\n",
    "            total_train_f1 += f1\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / num_batches_train\n",
    "        avg_train_accuracy = total_train_accuracy / num_batches_train\n",
    "        avg_train_iou = total_train_iou / num_batches_train\n",
    "        avg_train_precision = total_train_precision / num_batches_train\n",
    "        avg_train_f1 = total_train_f1 / num_batches_train\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_accuracy)\n",
    "        train_ious.append(avg_train_iou)\n",
    "        train_precisions.append(avg_train_precision)\n",
    "        train_f1s.append(avg_train_f1)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_valid_loss = 0.0\n",
    "        total_valid_accuracy, total_valid_iou, total_valid_precision, total_valid_f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        num_batches_valid = len(valid_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                images, masks = images.to(DEVICE), masks.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                total_valid_loss += loss.item()\n",
    "                acc, iou, precision, recall, f1 = calculate_metrics(outputs, masks)\n",
    "                total_valid_accuracy += acc\n",
    "                total_valid_iou += iou\n",
    "                total_valid_precision += precision\n",
    "                total_valid_f1 += f1\n",
    "\n",
    "        avg_valid_loss = total_valid_loss / num_batches_valid\n",
    "        avg_valid_accuracy = total_valid_accuracy / num_batches_valid\n",
    "        avg_valid_iou = total_valid_iou / num_batches_valid\n",
    "        avg_valid_precision = total_valid_precision / num_batches_valid\n",
    "        avg_valid_f1 = total_valid_f1 / num_batches_valid\n",
    "\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        valid_accuracies.append(avg_valid_accuracy)\n",
    "        valid_ious.append(avg_valid_iou)\n",
    "        valid_precisions.append(avg_valid_precision)\n",
    "        valid_f1s.append(avg_valid_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Acc: {avg_train_accuracy:.4f}, Train IoU: {avg_train_iou:.4f}, \"\n",
    "              f\"Train Precision: {avg_train_precision:.4f}, Train F1: {avg_train_f1:.4f}, \"\n",
    "              f\"Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {avg_valid_accuracy:.4f}, \"\n",
    "              f\"Valid IoU: {avg_valid_iou:.4f}, Valid Precision: {avg_valid_precision:.4f}, \"\n",
    "              f\"Valid F1: {avg_valid_f1:.4f}\")\n",
    "\n",
    "    return model, train_losses, train_accuracies, train_ious, train_precisions, train_f1s, valid_losses, valid_accuracies, valid_ious, valid_precisions, valid_f1s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55TYxuIVFC0D"
   },
   "source": [
    "Configures PyTorch's CUDA memory allocator to handle memory more efficiently when working with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_9XxbPxFC0D"
   },
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GzrzSauFC0E"
   },
   "source": [
    "Total Memory allocation for Unet (CNN) pior to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1739407052736,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "Ezk27rK5FC0F",
    "outputId": "5195e51a-943a-4633-d418-1c6898dad2bc"
   },
   "outputs": [],
   "source": [
    "model_memory_usage(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJo-bKnbFC0G"
   },
   "source": [
    "Train Unet (CNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4102029,
     "status": "ok",
     "timestamp": 1739411154753,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "Zs6nYp93FC0G",
    "outputId": "1683046f-7248-468a-ff46-6b3c648b8722"
   },
   "outputs": [],
   "source": [
    "# Train the U-Net model\n",
    "trained_model_basic_cnn, train_losses, train_accuracies, train_ious, train_precisions, train_f1s, valid_losses, valid_accuracies, valid_ious, valid_precisions, valid_f1s = train_model(\n",
    "    model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=40\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Epoch Used in Publication (U-Net)\n",
    "![image5.png](images/image5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG3nzaqGFC0H"
   },
   "source": [
    "How many Seconds it takes to Process a Singular Image (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58052,
     "status": "ok",
     "timestamp": 1739411212802,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "ZyfIbWMiFC0I",
    "outputId": "e3ca194a-79ef-4e3b-f171-dd35efb428a5"
   },
   "outputs": [],
   "source": [
    "# Measure Inference Time\n",
    "measure_inference_time(trained_model_basic_cnn, test_loader, DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time per image Used in Publication (U-Net)\n",
    "![image4.png](images/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmxW6skRFC0J"
   },
   "source": [
    "Total Memory allocation for Unet (CNN) after to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739411212802,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "CS-KlLo3FC0J",
    "outputId": "2544f356-342e-407a-fb08-6cab78070057"
   },
   "outputs": [],
   "source": [
    "# Measure Model Memory Usage\n",
    "model_memory_usage(trained_model_basic_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EY6QP4-FC0K"
   },
   "source": [
    "Save (CNN) Model state for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0EaeedkFC0L"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(trained_model_basic_cnn.state_dict(), \"unet_brain_tumor_segmentation.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTNnq4xCFC0L"
   },
   "source": [
    "Visualize the Location and size of the Brain Tumor based on image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5694,
     "status": "ok",
     "timestamp": 1739415285137,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "rv1Y0eA_FC0M",
    "outputId": "63be4fd8-1a10-466f-a68a-372cc4b6aa30"
   },
   "outputs": [],
   "source": [
    "# Printout image and Predicted Image Segmentation and True Segmentation\n",
    "def visualize_unet(input_image, output_image, target_image):\n",
    "    input_image = input_image.cpu().numpy().squeeze()\n",
    "    output_image = output_image.cpu().numpy().squeeze()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(input_image, cmap='gray')\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(output_image, cmap='gray')\n",
    "    axes[1].set_title('Output Image (Predicted)')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    sample_count = 0\n",
    "    max_samples = 5\n",
    "\n",
    "    for batch, (X, y) in enumerate(valid_loader):\n",
    "\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "        X = X.to(DEVICE, dtype=torch.float32)\n",
    "        y = y.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "        y_pred_logits = trained_model_basic_cnn(X)\n",
    "        y_pred_binary = (y_pred_logits > 0.5).float()\n",
    "        visualize_unet(X[0], y_pred_binary[0], y[0])\n",
    "        sample_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenation Used for Publication (Unet)\n",
    "![image.png](images/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tITJpxd9FC0O"
   },
   "source": [
    "### Hybrid CNN/Transformer Unet I call it SparseTransUnet\n",
    "\n",
    "Uses: Linformer attention layer to process the bottleneck part of the (CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukba-4NSFC0P"
   },
   "outputs": [],
   "source": [
    "# Define the Sparse Attention Block (Using Linformer for efficient self-attention)\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, seq_len, heads=1, depth=1, k=64):\n",
    "        super().__init__()\n",
    "        self.linformer = Linformer(\n",
    "            dim=dim, seq_len=seq_len, depth=depth, heads=heads, k=k\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linformer(x)\n",
    "\n",
    "# Define the SparseTransformer UNet Architecture\n",
    "class LinTransUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(LinTransUNet, self).__init__()\n",
    "        self.enc1 = DownsampleBlock(in_channels, 64)\n",
    "        self.enc2 = DownsampleBlock(64, 128)\n",
    "        self.enc3 = DownsampleBlock(128, 256)\n",
    "        self.enc4 = DownsampleBlock(256, 512)\n",
    "\n",
    "        # Transformer-based bottleneck\n",
    "        self.attention = AttentionBlock(dim=512, seq_len=16*16)\n",
    "        self.bottleneck = DoubleConvBlock(512, 1024)\n",
    "\n",
    "        self.dec4 = UpsampleBlock(1024, 512)\n",
    "        self.dec3 = UpsampleBlock(512, 256)\n",
    "        self.dec2 = UpsampleBlock(256, 128)\n",
    "        self.dec1 = UpsampleBlock(128, 64)\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1, p1 = self.enc1(x)\n",
    "        enc2, p2 = self.enc2(p1)\n",
    "        enc3, p3 = self.enc3(p2)\n",
    "        enc4, p4 = self.enc4(p3)\n",
    "\n",
    "        # Apply sparse transformer-based attention\n",
    "        b, c, h, w = p4.shape\n",
    "\n",
    "        # Convert to sequence\n",
    "        p4 = p4.flatten(2).transpose(1, 2)\n",
    "        p4 = self.attention(p4)\n",
    "\n",
    "        # Convert back to image shape\n",
    "        p4 = p4.transpose(1, 2).view(b, c, h, w)\n",
    "\n",
    "        bottleneck = self.bottleneck(p4)\n",
    "        dec4 = self.dec4(bottleneck, enc4)\n",
    "        dec3 = self.dec3(dec4, enc3)\n",
    "        dec2 = self.dec2(dec3, enc2)\n",
    "        dec1 = self.dec1(dec2, enc1)\n",
    "        return self.final_conv(dec1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzHCgmxPFC0Q"
   },
   "source": [
    "Architecture of the Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1739411219137,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "T9pC7IJ1FC0Q",
    "outputId": "5cd25818-9f6b-4a52-e717-13edded1a987"
   },
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "lin_model = LinTransUNet(in_channels=1, out_channels=1).to(DEVICE)\n",
    "summary(lin_model, (1, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9zJGFbmFC0R"
   },
   "source": [
    "SparseTransUnet's Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "357_gaIKFC0S"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(lin_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUmFVWrWFC0S"
   },
   "source": [
    "Total Memory allocation for Unet Hybrid (Transformer-CNN) pior to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1739411219138,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "mxe99RBrFC0T",
    "outputId": "e99c6c93-2deb-4897-d419-87dd975d9937"
   },
   "outputs": [],
   "source": [
    "model_memory_usage(lin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0ONJqGMFC0U"
   },
   "source": [
    "Hybrid model's Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3861126,
     "status": "ok",
     "timestamp": 1739415080257,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "xGUqTAIbFC0V",
    "outputId": "904d1726-69f3-4d49-e845-a65878ef24fb"
   },
   "outputs": [],
   "source": [
    "# Train the U-Net model\n",
    "trained_model_lin, train_losses_lin, train_accuracies_lin, train_ious_lin, train_precisions_lin, train_f1s_lin, valid_losses_lin, valid_accuracies_lin, valid_ious_lin, valid_precisions_lin, valid_f1s_lin = train_model(\n",
    "    lin_model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Epoch Used in Publication (LinTUNet)\n",
    "![image2.png](images/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bh-TG8omFC0W"
   },
   "source": [
    "How many Seconds it takes to Process a Singular Image (Transformer-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3528,
     "status": "ok",
     "timestamp": 1739415083780,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "C6TwZkh1FC0W",
    "outputId": "7062da73-5eee-4917-83d8-93f20402f416"
   },
   "outputs": [],
   "source": [
    "# Measure Inference Time\n",
    "measure_inference_time(trained_model_lin, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time per image Used in Publication (LinTUNet)\n",
    "![image3.png](images/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5fpRt3PFC0X"
   },
   "source": [
    "Total Memory allocation for Unet Hybrid (Transformer-CNN) after to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739415083780,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "ure-8n3VFC0Y",
    "outputId": "c05817ca-209b-4015-bd7e-f5498c112d93"
   },
   "outputs": [],
   "source": [
    "# Measure Model Memory Usage\n",
    "model_memory_usage(trained_model_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbXK9mHzFC0Z"
   },
   "source": [
    "Visualization of The hydbrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5429,
     "status": "ok",
     "timestamp": 1739415089204,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "O-bP68P2FC0Z",
    "outputId": "a4989aca-96ea-476c-cdc1-ae9f9d42226c"
   },
   "outputs": [],
   "source": [
    "# Printout image and Predicted Image Segmentation and True Segmentation\n",
    "def visualize_unet(input_image, output_image, target_image):\n",
    "    input_image = input_image.cpu().numpy().squeeze()\n",
    "    output_image = output_image.cpu().numpy().squeeze()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(input_image, cmap='gray')\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(output_image, cmap='gray')\n",
    "    axes[1].set_title('Output Image (Predicted)')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(\"output.png\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    sample_count = 0\n",
    "    max_samples = 5\n",
    "\n",
    "    for batch, (X, y) in enumerate(valid_loader):\n",
    "\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "        X = X.to(DEVICE, dtype=torch.float32)\n",
    "        y = y.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "        y_pred_logits = trained_model_lin(X)\n",
    "        y_pred_binary = (y_pred_logits > 0.5).float()\n",
    "        visualize_unet(X[0], y_pred_binary[0], y[0])\n",
    "        sample_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenation Used for Publication (LinTUnet)\n",
    "![image10.png](images/image10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LqjVL7dFC0a"
   },
   "source": [
    "Plotting the Loss function and Accuracy of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxq5AUtAFC0b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert accuracy lists to CPU and then to NumPy arrays\n",
    "train_accuracies_np = np.array([acc.cpu().numpy() if isinstance(acc, torch.Tensor) else acc for acc in train_accuracies])\n",
    "valid_accuracies_np = np.array([acc.cpu().numpy() if isinstance(acc, torch.Tensor) else acc for acc in valid_accuracies])\n",
    "\n",
    "train_accuracies_lin = np.array([acc.cpu().numpy() if isinstance(acc, torch.Tensor) else acc for acc in train_accuracies_lin])\n",
    "valid_accuracies_lin = np.array([acc.cpu().numpy() if isinstance(acc, torch.Tensor) else acc for acc in valid_accuracies_lin])\n",
    "\n",
    "# Convert loss lists to NumPy arrays\n",
    "train_losses_np = np.array(train_losses)\n",
    "valid_losses_np = np.array(valid_losses)\n",
    "train_losses_lin = np.array(train_losses_lin)\n",
    "valid_losses_lin = np.array(valid_losses_lin)\n",
    "\n",
    "# Define fixed y-axis ranges\n",
    "min_epoch = 0  # Assuming epochs start at 0\n",
    "max_epoch = 35  # Set max based on number of epochs\n",
    "\n",
    "\n",
    "min_loss = min(train_losses_np.min(), valid_losses_np.min(), train_losses_lin.min(), valid_losses_lin.min())\n",
    "max_loss = 0.6  # Upper limit fixed at 0.6\n",
    "min_acc = 0.8  # Accuracy starts from 0\n",
    "max_acc = 1.0  # Upper limit fixed at 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sE4yxcmRDrH"
   },
   "source": [
    "CNN loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739416546031,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "0OlgLQu-P-e-",
    "outputId": "a6f91b30-ae98-4be9-d444-c7a2029983e4"
   },
   "outputs": [],
   "source": [
    "# Create plots with fixed axis scaling\n",
    "fig, axes = plt.subplots(1, figsize=(10, 5))\n",
    "\n",
    "# Plot Loss\n",
    "axes.plot(train_losses_np, label='Train Loss')\n",
    "axes.plot(valid_losses_np, label='Validation Loss')\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.set_title('Training and Validation Loss')\n",
    "axes.legend()\n",
    "axes.set_ylim(min_loss, max_loss)  # Set fixed y-axis limits\n",
    "axes.set_xlim(min_epoch, max_epoch)  # CNN Loss\n",
    "axes.set_aspect(1.0 / axes.get_data_ratio())  # Make box square\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Used in Publication (U-Net) \n",
    "![image6.png](images/image6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1K8GNGe-Q-us"
   },
   "source": [
    "Hybrid Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1739416549664,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "HdSRHjvIP-MQ",
    "outputId": "bc5f56ab-e8c0-4253-e832-c3093e459405"
   },
   "outputs": [],
   "source": [
    "# Create plots with fixed axis scaling\n",
    "fig, axes = plt.subplots(1, figsize=(10, 5))\n",
    "\n",
    "# Plot Loss\n",
    "axes.plot(train_losses_lin, linestyle='dashed', label='Train Loss')\n",
    "axes.plot(valid_losses_lin, linestyle='dashed', label='Validation Loss')\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.set_title('Training and Validation Loss')\n",
    "axes.legend()\n",
    "axes.set_ylim(min_loss, max_loss)  # Set fixed y-axis limits\n",
    "axes.set_xlim(min_epoch, max_epoch) # fixed x-axis\n",
    "axes.set_aspect(1.0 / axes.get_data_ratio())  # Make box square\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Used in Publication (LinTUNet) \n",
    "![image7.png](images/image7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhdY0vTRQ7if"
   },
   "source": [
    "CNN Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1739416558371,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "ouRzLGz7P9q-",
    "outputId": "25af7b80-63cb-4e31-cca5-e4a23e8657af"
   },
   "outputs": [],
   "source": [
    "# Create plots with fixed axis scaling\n",
    "fig, axes = plt.subplots(1, figsize=(10, 5))\n",
    "\n",
    "# Plot Accuracy\n",
    "axes.plot(train_accuracies_np, label='Train Accuracy')\n",
    "axes.plot(valid_accuracies_np, label='Validation Accuracy')\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Accuracy')\n",
    "axes.set_title('Training and Validation Accuracy')\n",
    "axes.legend()\n",
    "axes.set_ylim(min_acc, max_acc)  # Set fixed y-axis limits\n",
    "axes.set_xlim(min_epoch, max_epoch)\n",
    "axes.set_aspect(1.0 / axes.get_data_ratio())  # Make box square\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Used in Publication (U-Net)\n",
    "![image8.png](images/image8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doWWU0ooRGXk"
   },
   "source": [
    "Hybrid Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1739416562487,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "VxmpVpEjQTih",
    "outputId": "38378e1e-4940-4798-ff35-a91757c01341"
   },
   "outputs": [],
   "source": [
    "# Create plots with fixed axis scaling\n",
    "fig, axes = plt.subplots(1,figsize=(10, 5))\n",
    "\n",
    "# Plot Accuracy\n",
    "axes.plot(train_accuracies_lin, linestyle='dashed', label='Train Accuracy ')\n",
    "axes.plot(valid_accuracies_lin, linestyle='dashed', label='Validation Accuracy ')\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Accuracy')\n",
    "axes.set_title('Training and Validation Accuracy')\n",
    "axes.legend()\n",
    "axes.set_ylim(min_acc, max_acc)  # Set fixed y-axis limits\n",
    "axes.set_xlim(min_epoch, max_epoch)\n",
    "axes.set_aspect(1.0 / axes.get_data_ratio())  # Make box square\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Used in Publication (LinTUNet)\n",
    "![image9.png](images/image9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEsEDSiNFC0b"
   },
   "source": [
    "Comparisons of the Accuracies and IoU's of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1739415204333,
     "user": {
      "displayName": "Lawrence Menegus",
      "userId": "11984566026369855003"
     },
     "user_tz": 300
    },
    "id": "u-Vyc5VSG983",
    "outputId": "8508e23a-97c3-44c2-dc6a-6aa8f5d16792"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Metric\": [\"F1 Score (Train)\", \"Accuracy (Train)\", \"IoU (Train)\", \"Precision (Train)\"],\n",
    "    \"Unet (CNN)\": [train_f1s[-1], train_accuracies_np[-1], train_ious[-1], train_precisions[-1]],\n",
    "    \"Hybrid Unet (Transformer-CNN)\": [train_f1s_lin[-1], train_accuracies_lin[-1], train_ious_lin[-1], train_precisions[-1]]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "tools.display_dataframe_to_user(name = \" \" , dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Used in Publication\n",
    "![image11.png](images/image11.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
